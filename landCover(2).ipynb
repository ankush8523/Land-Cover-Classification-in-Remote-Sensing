{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankush8523/Land-Cover-Classification-in-Remote-Sensing/blob/main/landCover(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjSUXVJMVjsY",
        "outputId": "28d73b4a-795a-44e4-c0b4-22f627e7581f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spectral\n",
            "  Downloading spectral-0.23.1-py3-none-any.whl (212 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/212.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.2/212.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.9/212.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from spectral) (1.25.2)\n",
            "Installing collected packages: spectral\n",
            "Successfully installed spectral-0.23.1\n"
          ]
        }
      ],
      "source": [
        "pip install spectral"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install np_utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crcIFyUnWBOs",
        "outputId": "41e58e01-87d1-4b53-f335-3d5972067ab7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting np_utils\n",
            "  Downloading np_utils-0.6.0.tar.gz (61 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/62.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.0 in /usr/local/lib/python3.10/dist-packages (from np_utils) (1.25.2)\n",
            "Building wheels for collected packages: np_utils\n",
            "  Building wheel for np_utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for np_utils: filename=np_utils-0.6.0-py3-none-any.whl size=56441 sha256=df7099ebff478982f0fcb209fb6452155e468b03d4e280d725781ff314d004d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/b6/c7/50/2307607f44366dd021209f660045f8d51cb976514d30be7cc7\n",
            "Successfully built np_utils\n",
            "Installing collected packages: np_utils\n",
            "Successfully installed np_utils-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.layers import Dropout, Input, GlobalAveragePooling2D\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "# from keras.utils import np_utils\n",
        "from keras import utils\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score\n",
        "from operator import truediv\n",
        "from plotly.offline import init_notebook_mode\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as sio\n",
        "import os\n",
        "import spectral\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from scipy.io import loadmat\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Reshape\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dropout, Input, AveragePooling2D, Activation, MaxPooling2D, BatchNormalization, Concatenate\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
        "from numpy.random import seed\n",
        "from time import time\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.metrics import Precision, Recall, Accuracy\n",
        "from tensorflow.keras import optimizers, regularizers\n",
        "import cv2\n",
        "seed(11)\n",
        "init_notebook_mode(connected=True)\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "IKe3cDlRWDbI",
        "outputId": "250c10e0-b445-47c9-f568-b7f422e70e2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-2.24.1.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['KERAS_BACKEND']='tensorflow'"
      ],
      "metadata": {
        "id": "L_aUrfPcWGN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loadData(name):\n",
        "\n",
        "    data_path = os.path.join(os.getcwd(),'/content/drive/MyDrive/dataset/')\n",
        "\n",
        "    if name == 'Houston':\n",
        "\n",
        "        MS = sio.loadmat(os.paath.join(data_path, 'HS-MS Houston2013/data_MS_HR.mat'))['data_MS_HR']\n",
        "        HS= sio.loadmat(os.path.join(data_path, 'HS-MS Houston2013/data_HS_LR.mat'))['data_HS_LR']\n",
        "        Train = sio.loadmat(os.path.join(data_path, 'HS-MS Houston2013/Train.mat'))['Train']\n",
        "        Test = sio.loadmat(os.path.join(data_path, 'HS-MS Houston2013/Validation.mat'))['Validation']\n",
        "\n",
        "    if name == 'Berlin':\n",
        "\n",
        "        SAR = sio.loadmat(os.path.join(data_path, 'BerlinDataset/data_SAR_HR.mat'))['data_SAR_HR']\n",
        "        HS = sio.loadmat(os.path.join(data_path, 'BerlinDataset/data_HS_LR.mat'))['data_HS_LR']\n",
        "        Train= sio.loadmat(os.path.join(data_path, 'BerlinDataset/TrainImage.mat'))['TrainImage']\n",
        "        Test = sio.loadmat(os.path.join(data_path, 'BerlinDataset/TestImage.mat'))['TestImage']\n",
        "\n",
        "    if name == 'Augsburg':\n",
        "\n",
        "        SAR = sio.loadmat(os.path.join(data_path, 'Augsburg/data_SAR_HR.mat'))['data_SAR_HR']\n",
        "        HS = sio.loadmat(os.path.join(data_path, 'Augsburg/data_HS_LR.mat'))['data_HS_LR']\n",
        "        DSM = sio.loadmat(os.path.join(data_path, 'Augsburg/data_DSM.mat'))['data_DSM']\n",
        "        Train= sio.loadmat(os.path.join(data_path, 'Augsburg/TrainImage.mat'))['TrainImage']\n",
        "        Test = sio.loadmat(os.path.join(data_path, 'Augsburg/TestImage.mat'))['TestImage']\n",
        "\n",
        "    return SAR, HS, DSM, Train, Test\n"
      ],
      "metadata": {
        "id": "wnOv0sILWLOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## GLOBAL VARIABLES\n",
        "windowSize = 8"
      ],
      "metadata": {
        "id": "_dk6n3_rWMO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def applyPCA(X, numComponents=75):\n",
        "    newX = np.reshape(X, (-1, X.shape[2]))\n",
        "    pca = PCA(n_components=numComponents, whiten=True)\n",
        "    newX = pca.fit_transform(newX)\n",
        "    newX = np.reshape(newX, (X.shape[0],X.shape[1], numComponents))\n",
        "    return newX, pca"
      ],
      "metadata": {
        "id": "NMZwGYZuWOQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def splitTrainTestSet(X, y, testRatio, randomState=345):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testRatio, random_state=randomState,\n",
        "                                                        stratify=y)\n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "metadata": {
        "id": "A3GGzw-ZWQdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def padWithZeros(X, margin=2):\n",
        "    newX = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2* margin, X.shape[2]))\n",
        "    x_offset = margin\n",
        "    y_offset = margin\n",
        "    newX[x_offset:X.shape[0] + x_offset, y_offset:X.shape[1] + y_offset, :] = X\n",
        "    return newX"
      ],
      "metadata": {
        "id": "IQuST3MQWSaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def createImageCubes(X, y, windowSize=9, removeZeroLabels = True):\n",
        "    margin = int((windowSize) / 2)\n",
        "    zeroPaddedX = padWithZeros(X, margin=margin)\n",
        "    # split patches\n",
        "    patchesData = np.zeros((X.shape[0] * X.shape[1], windowSize, windowSize, X.shape[2]))\n",
        "    patchesLabels = np.zeros((X.shape[0] * X.shape[1]))\n",
        "    patchIndex = 0\n",
        "    for r in range(margin, zeroPaddedX.shape[0] - margin):\n",
        "        for c in range(margin, zeroPaddedX.shape[1] - margin):\n",
        "            patch = zeroPaddedX[r - margin:r + margin , c - margin:c + margin ]\n",
        "            patchesData[patchIndex, :, :, :] = patch\n",
        "            patchesLabels[patchIndex] = y[r-margin, c-margin]\n",
        "            patchIndex = patchIndex + 1\n",
        "    if removeZeroLabels:\n",
        "        patchesData = patchesData[patchesLabels>0,:,:,:]\n",
        "        patchesLabels = patchesLabels[patchesLabels>0]\n",
        "        patchesLabels -= 1\n",
        "    return patchesData, patchesLabels"
      ],
      "metadata": {
        "id": "I32-kr6RWUh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = 'Augsburg'\n",
        "SAR, HS , DSM, Train, Test = loadData(dataset)\n",
        "\n",
        "K = 15 if dataset == 'Berlin' else 12\n",
        "HSr, pca = applyPCA(HS, numComponents=K)\n",
        "DSM= DSM.reshape((DSM.shape[0],DSM.shape[1],1))"
      ],
      "metadata": {
        "id": "GxTmtUdVWWqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "XTrain1, Train1 = createImageCubes(SAR, Train, windowSize=windowSize)\n",
        "XTest1, Test1 = createImageCubes(SAR, Test, windowSize=windowSize)\n",
        "\n",
        "XTrain2, Train2 = createImageCubes(HSr, Train, windowSize=windowSize)\n",
        "XTest2, Test2 = createImageCubes(HSr, Test, windowSize=windowSize)\n",
        "\n",
        "XTrain3, Train3 = createImageCubes(DSM, Train, windowSize=windowSize)\n",
        "XTest3, Test3 = createImageCubes(DSM, Test, windowSize=windowSize)\n",
        "\n",
        "XtrainS = np.concatenate((XTrain1, XTrain2, XTrain3) , axis = 3)\n",
        "XtestS = np.concatenate((XTest1, XTest2, XTest3) , axis = 3)\n",
        "\n",
        "YtrainS = Train1\n",
        "YtestS = Test1\n",
        "YtrainS = utils.to_categorical(YtrainS)"
      ],
      "metadata": {
        "id": "6G53go0wWZAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras-cv-attention-models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mL-x7H71Wb6P",
        "outputId": "4cdbd876-e673-4ac1-d6d7-ae91cdfb0a7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-cv-attention-models\n",
            "  Downloading keras_cv_attention_models-1.4.1-py3-none-any.whl (796 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.3/796.3 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from keras-cv-attention-models) (9.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from keras-cv-attention-models) (4.66.2)\n",
            "Collecting ftfy (from keras-cv-attention-models)\n",
            "  Downloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from keras-cv-attention-models) (2023.12.25)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (from keras-cv-attention-models) (4.9.4)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from keras-cv-attention-models) (2.15.0)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->keras-cv-attention-models) (0.2.13)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-cv-attention-models) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-cv-attention-models) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-cv-attention-models) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-cv-attention-models) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-cv-attention-models) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-cv-attention-models) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-cv-attention-models) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-cv-attention-models) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-cv-attention-models) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-cv-attention-models) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-cv-attention-models) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-cv-attention-models) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-cv-attention-models) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-cv-attention-models) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-cv-attention-models) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-cv-attention-models) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-cv-attention-models) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-cv-attention-models) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-cv-attention-models) (1.62.2)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-cv-attention-models) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-cv-attention-models) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-cv-attention-models) (2.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv-attention-models) (8.1.7)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv-attention-models) (0.1.8)\n",
            "Requirement already satisfied: etils[enp,epath,etree]>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv-attention-models) (1.7.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv-attention-models) (2.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv-attention-models) (5.9.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv-attention-models) (2.31.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv-attention-models) (1.14.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv-attention-models) (0.10.2)\n",
            "Requirement already satisfied: array-record>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv-attention-models) (0.5.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->keras-cv-attention-models) (0.43.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->keras-cv-attention-models) (2023.6.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->keras-cv-attention-models) (6.4.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->keras-cv-attention-models) (3.18.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets->keras-cv-attention-models) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets->keras-cv-attention-models) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets->keras-cv-attention-models) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets->keras-cv-attention-models) (2024.2.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->keras-cv-attention-models) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->keras-cv-attention-models) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->keras-cv-attention-models) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->keras-cv-attention-models) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->keras-cv-attention-models) (3.0.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-metadata->tensorflow-datasets->keras-cv-attention-models) (1.63.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->keras-cv-attention-models) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->keras-cv-attention-models) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->keras-cv-attention-models) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow->keras-cv-attention-models) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow->keras-cv-attention-models) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->keras-cv-attention-models) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow->keras-cv-attention-models) (3.2.2)\n",
            "Installing collected packages: ftfy, keras-cv-attention-models\n",
            "Successfully installed ftfy-6.2.0 keras-cv-attention-models-1.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_outputs=7\n",
        "\n",
        "from keras_cv_attention_models import attention_layers\n",
        "image_size=8\n",
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.Resizing(image_size, image_size),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(factor=0.02),\n",
        "        layers.RandomZoom(\n",
        "            height_factor=0.2, width_factor=0.2\n",
        "        ),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")\n",
        "# Compute the mean and the variance of the training data for normalization.\n",
        "data_augmentation.layers[0].adapt(XtrainS)"
      ],
      "metadata": {
        "id": "AFOwkzKvWiCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def AttentionGCN():\n",
        "\n",
        "    input_layer = Input(shape=(8,8,17), name='input')\n",
        "    #inputs = data_augmentation(input_layer)\n",
        "    conv1 = Conv2D(8, kernel_size=1, activation='relu', name='conv1', kernel_regularizer=regularizers.l2(0.0005))(input_layer)\n",
        "\n",
        "    nn = attention_layers.cot_attention(conv1, kernel_size=3)\n",
        "\n",
        "    #pool1 = MaxPooling2D(pool_size=(2,2), name='pool1')(nn)\n",
        "    # nn1 = attention_layers.neighborhood_attention(conv1, kernel_size=3)\n",
        "    # nn2 = attention_layers.neighborhood_attention(conv1,kernel_size=5)\n",
        "    # concat = layers.concatenate([nn1, nn2], axis =-1)\n",
        "    # drop = Dropout(0.2)(concat)\n",
        "    # multi_Attent = Conv2D(8, kernel_size=1, activation='relu', name='multi_Attent', kernel_regularizer=regularizers.l2(0.0005))(drop)\n",
        "\n",
        "\n",
        "# ########## Graph-based global reasoning module # input is 8x8x8\n",
        "\n",
        "    graph_t = Reshape((8, 8*8))(nn)\n",
        "    squeezed_graph_t = Conv1D(16, 1, activation='relu', name='squeezer', kernel_regularizer=regularizers.l2(0.0001))(graph_t)\n",
        "    squeezed_graph = Reshape((8, 16))(squeezed_graph_t)\n",
        "    gconv = Conv1D(64, 1, activation='relu', name='gconv', kernel_regularizer=regularizers.l2(0.0001))(squeezed_graph)\n",
        "    gconv_t = Reshape((8, 64))(gconv)\n",
        "    unsqueezed_graph_transpose = Conv1D(64, 1, activation='relu', name='unsqueezer', kernel_regularizer=regularizers.l2(0.0001))(gconv_t)\n",
        "    unsqueezed_graph = Reshape((64, 8))(unsqueezed_graph_transpose)\n",
        "    glore = Reshape((8, 8, 8))(unsqueezed_graph)\n",
        "\n",
        "    # Proposed Block\n",
        "    nn1 = attention_layers.neighborhood_attention(glore, kernel_size=3)\n",
        "    nn2 = attention_layers.neighborhood_attention(glore, kernel_size=5)\n",
        "    concat1 = layers.concatenate([nn1, nn2], axis =-1)\n",
        "    drop1 = Dropout(0.2)(concat1)\n",
        "    multi_Attent1 = Conv2D(8, kernel_size=1, activation='relu', name='multi_Attent1', kernel_regularizer=regularizers.l2(0.0005))(drop1)\n",
        "\n",
        "    # nn1 =  attention_layers.neighborhood_attention(glore, kernel_size=3)\n",
        "\n",
        "    block1 = layers.concatenate([conv1, multi_Attent1], axis=-1) #################  you do not need to add the feature maps of the conv1 layer\n",
        "    block12 = layers.concatenate([conv1, block1,nn ], axis=-1) #################  you do not need to use the feature maps of the attention block\n",
        "    #pool2 = MaxPooling2D(pool_size=(2,2), name='pool2')(block1)\n",
        "    conv2 = Conv2D(32, kernel_size=1, activation='relu', name='conv2', kernel_regularizer=regularizers.l2(0.0005))(block12)\n",
        "\n",
        "# Graph-based global reasoning module 2 # input is 8x8x32\n",
        "\n",
        "    graph_t2 = Reshape((32,8*8))(conv2)\n",
        "    squeezed_graph_t2 = Conv1D(64, 1, activation='relu', name='squeezer2', kernel_regularizer=regularizers.l2(0.0001))(graph_t2)\n",
        "    squeezed_graph2 = Reshape((64, 32))(squeezed_graph_t2)\n",
        "    gconv2 = Conv1D(32, 1, activation='relu', name='gconv2', kernel_regularizer=regularizers.l2(0.0001))(squeezed_graph2)\n",
        "    gconv_transpose2 = Reshape((64, 32))(gconv2)\n",
        "    unsqueezed_graph_t2 = Conv1D(8*8, 1, activation='relu', name='unsqueezer2', kernel_regularizer=regularizers.l2(0.0001))(gconv_transpose2)\n",
        "    unsqueezed_graph2 = Reshape((8*8, 64))(unsqueezed_graph_t2)\n",
        "    glore2 = Reshape((8, 8, 64))(unsqueezed_graph2)\n",
        "\n",
        "    # nn2 =  attention_layers.neighborhood_attention(glore2, kernel_size=3)\n",
        "\n",
        "    # Proposed Block\n",
        "    nn3 = attention_layers.neighborhood_attention(glore2, kernel_size=3)\n",
        "    nn4 = attention_layers.neighborhood_attention(glore2, kernel_size=5)\n",
        "    concat2 = layers.concatenate([nn3, nn4], axis =-1)\n",
        "    drop2 = Dropout(0.2)(concat2)\n",
        "    multi_Attent2 = Conv2D(8, kernel_size=1, activation='relu', name='multi_Attent2', kernel_regularizer=regularizers.l2(0.0005))(drop2)\n",
        "\n",
        "    block2 = layers.concatenate([conv2, glore2], axis=-1) #################  you do not need to add the feature maps of the conv2 layer\n",
        "    block21 = layers.concatenate([conv2, block2, multi_Attent2], axis=-1) #################  you do not need to use the feature maps of the attention block2\n",
        "\n",
        "    conv3 = Conv2D(32, kernel_size=1, activation='relu', name='conv3', kernel_regularizer=regularizers.l2(0.0005))(block21)\n",
        "    conv4 = Conv2D(64, kernel_size=3, activation='relu', name='conv4', kernel_regularizer=regularizers.l2(0.0005))(conv3)\n",
        "    conv5 = Conv2D(128, kernel_size=1, activation='relu', name='conv5', kernel_regularizer=regularizers.l2(0.0005))(conv4)\n",
        "\n",
        "    # flat = Flatten()(conv5)\n",
        "    gap = GlobalAveragePooling2D()(conv5)\n",
        "\n",
        "    fc1 = Dense(100, activation='relu', name='fc1', kernel_regularizer=regularizers.l2(0.01))(gap)\n",
        "    fc2 = Dense(20, activation='relu', name='fc2', kernel_regularizer=regularizers.l2(0.01))(fc1)\n",
        "    fc3 = Dense(7, activation='softmax', name='fc3', kernel_regularizer=regularizers.l2(0.0001))(fc2)\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=fc3, name='CNN_model_with_global_reasoning_module')\n",
        "\n",
        "    return model\n",
        "\n",
        "# define the model with input layer and output layer\n",
        "model = AttentionGCN()"
      ],
      "metadata": {
        "id": "K7apItRjYBs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gk6oU3D-YT3H",
        "outputId": "b85b19d0-67c7-47c3-cc41-4c4cf1ddb35a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"CNN_model_with_global_reasoning_module\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input (InputLayer)          [(None, 8, 8, 17)]           0         []                            \n",
            "                                                                                                  \n",
            " conv1 (Conv2D)              (None, 8, 8, 8)              144       ['input[0][0]']               \n",
            "                                                                                                  \n",
            " zero_padding2d_1 (ZeroPadd  (None, 10, 10, 8)            0         ['conv1[0][0]']               \n",
            " ing2D)                                                                                           \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)           (None, 8, 8, 8)              144       ['zero_padding2d_1[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_5 (Bat  (None, 8, 8, 8)              32        ['conv2d_6[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_4 (Activation)   (None, 8, 8, 8)              0         ['batch_normalization_5[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " concatenate_8 (Concatenate  (None, 8, 8, 16)             0         ['conv1[0][0]',               \n",
            " )                                                                   'activation_4[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)           (None, 8, 8, 4)              64        ['concatenate_8[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_6 (Bat  (None, 8, 8, 4)              16        ['conv2d_7[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_5 (Activation)   (None, 8, 8, 4)              0         ['batch_normalization_6[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)           (None, 8, 8, 9)              45        ['activation_5[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)           (None, 8, 8, 8)              64        ['conv1[0][0]']               \n",
            "                                                                                                  \n",
            " group_normalization_1 (Gro  (None, 8, 8, 9)              18        ['conv2d_8[0][0]']            \n",
            " upNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_7 (Bat  (None, 8, 8, 8)              32        ['conv2d_9[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " tf.reshape_32 (TFOpLambda)  (None, 8, 8, 1, 9)           0         ['group_normalization_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " compatible_extract_patches  (None, 8, 8, 72)             0         ['batch_normalization_7[0][0]'\n",
            " _5 (CompatibleExtractPatch                                         ]                             \n",
            " es)                                                                                              \n",
            "                                                                                                  \n",
            " tf.compat.v1.transpose_9 (  (None, 8, 8, 9, 1)           0         ['tf.reshape_32[0][0]']       \n",
            " TFOpLambda)                                                                                      \n",
            "                                                                                                  \n",
            " tf.reshape_33 (TFOpLambda)  (None, 8, 8, 9, 8, 1)        0         ['compatible_extract_patches_5\n",
            "                                                                    [0][0]']                      \n",
            "                                                                                                  \n",
            " tf.expand_dims_7 (TFOpLamb  (None, 8, 8, 9, 1, 1)        0         ['tf.compat.v1.transpose_9[0][\n",
            " da)                                                                0]']                          \n",
            "                                                                                                  \n",
            " multiply_2 (Multiply)       (None, 8, 8, 9, 8, 1)        0         ['tf.reshape_33[0][0]',       \n",
            "                                                                     'tf.expand_dims_7[0][0]']    \n",
            "                                                                                                  \n",
            " tf.math.reduce_sum_2 (TFOp  (None, 8, 8, 8, 1)           0         ['multiply_2[0][0]']          \n",
            " Lambda)                                                                                          \n",
            "                                                                                                  \n",
            " tf.reshape_34 (TFOpLambda)  (None, 8, 8, 8)              0         ['tf.math.reduce_sum_2[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_8 (Bat  (None, 8, 8, 8)              32        ['tf.reshape_34[0][0]']       \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_6 (Activation)   (None, 8, 8, 8)              0         ['batch_normalization_8[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " add_1 (Add)                 (None, 8, 8, 8)              0         ['activation_6[0][0]',        \n",
            "                                                                     'activation_4[0][0]']        \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_1 (TFO  (None, 1, 1, 8)              0         ['add_1[0][0]']               \n",
            " pLambda)                                                                                         \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)          (None, 1, 1, 32)             288       ['tf.math.reduce_mean_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " batch_normalization_9 (Bat  (None, 1, 1, 32)             128       ['conv2d_10[0][0]']           \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_7 (Activation)   (None, 1, 1, 32)             0         ['batch_normalization_9[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)          (None, 1, 1, 16)             528       ['activation_7[0][0]']        \n",
            "                                                                                                  \n",
            " tf.expand_dims_8 (TFOpLamb  (None, 8, 8, 8, 1)           0         ['activation_6[0][0]']        \n",
            " da)                                                                                              \n",
            "                                                                                                  \n",
            " tf.expand_dims_9 (TFOpLamb  (None, 8, 8, 8, 1)           0         ['activation_4[0][0]']        \n",
            " da)                                                                                              \n",
            "                                                                                                  \n",
            " tf.reshape_35 (TFOpLambda)  (None, 1, 1, 8, 2)           0         ['conv2d_11[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_9 (Concatenate  (None, 8, 8, 8, 2)           0         ['tf.expand_dims_8[0][0]',    \n",
            " )                                                                   'tf.expand_dims_9[0][0]']    \n",
            "                                                                                                  \n",
            " softmax_5 (Softmax)         (None, 1, 1, 8, 2)           0         ['tf.reshape_35[0][0]']       \n",
            "                                                                                                  \n",
            " multiply_3 (Multiply)       (None, 8, 8, 8, 2)           0         ['concatenate_9[0][0]',       \n",
            "                                                                     'softmax_5[0][0]']           \n",
            "                                                                                                  \n",
            " tf.math.reduce_sum_3 (TFOp  (None, 8, 8, 8)              0         ['multiply_3[0][0]']          \n",
            " Lambda)                                                                                          \n",
            "                                                                                                  \n",
            " reshape_10 (Reshape)        (None, 8, 64)                0         ['tf.math.reduce_sum_3[0][0]']\n",
            "                                                                                                  \n",
            " squeezer (Conv1D)           (None, 8, 16)                1040      ['reshape_10[0][0]']          \n",
            "                                                                                                  \n",
            " reshape_11 (Reshape)        (None, 8, 16)                0         ['squeezer[0][0]']            \n",
            "                                                                                                  \n",
            " gconv (Conv1D)              (None, 8, 64)                1088      ['reshape_11[0][0]']          \n",
            "                                                                                                  \n",
            " reshape_12 (Reshape)        (None, 8, 64)                0         ['gconv[0][0]']               \n",
            "                                                                                                  \n",
            " unsqueezer (Conv1D)         (None, 8, 64)                4160      ['reshape_12[0][0]']          \n",
            "                                                                                                  \n",
            " reshape_13 (Reshape)        (None, 64, 8)                0         ['unsqueezer[0][0]']          \n",
            "                                                                                                  \n",
            " reshape_14 (Reshape)        (None, 8, 8, 8)              0         ['reshape_13[0][0]']          \n",
            "                                                                                                  \n",
            " dense_8 (Dense)             (None, 8, 8, 24)             216       ['reshape_14[0][0]']          \n",
            "                                                                                                  \n",
            " dense_10 (Dense)            (None, 8, 8, 24)             216       ['reshape_14[0][0]']          \n",
            "                                                                                                  \n",
            " tf.split_8 (TFOpLambda)     [(None, 8, 8, 8),            0         ['dense_8[0][0]']             \n",
            "                              (None, 8, 8, 16)]                                                   \n",
            "                                                                                                  \n",
            " tf.split_10 (TFOpLambda)    [(None, 8, 8, 8),            0         ['dense_10[0][0]']            \n",
            "                              (None, 8, 8, 16)]                                                   \n",
            "                                                                                                  \n",
            " compatible_extract_patches  (None, 6, 6, 3, 3, 16)       0         ['tf.split_8[0][1]']          \n",
            " _6 (CompatibleExtractPatch                                                                       \n",
            " es)                                                                                              \n",
            "                                                                                                  \n",
            " compatible_extract_patches  (None, 4, 4, 5, 5, 16)       0         ['tf.split_10[0][1]']         \n",
            " _7 (CompatibleExtractPatch                                                                       \n",
            " es)                                                                                              \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1  (None, 1, 6, 3, 3, 16)       0         ['compatible_extract_patches_6\n",
            " 6 (SlicingOpLambda)                                                [0][0]']                      \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1  (None, 1, 6, 3, 3, 16)       0         ['compatible_extract_patches_6\n",
            " 7 (SlicingOpLambda)                                                [0][0]']                      \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_2  (None, 1, 4, 5, 5, 16)       0         ['compatible_extract_patches_7\n",
            " 0 (SlicingOpLambda)                                                [0][0]']                      \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_2  (None, 1, 4, 5, 5, 16)       0         ['compatible_extract_patches_7\n",
            " 1 (SlicingOpLambda)                                                [0][0]']                      \n",
            "                                                                                                  \n",
            " tf.repeat_16 (TFOpLambda)   (None, 1, 6, 3, 3, 16)       0         ['tf.__operators__.getitem_16[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " tf.repeat_17 (TFOpLambda)   (None, 1, 6, 3, 3, 16)       0         ['tf.__operators__.getitem_17[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " tf.repeat_20 (TFOpLambda)   (None, 2, 4, 5, 5, 16)       0         ['tf.__operators__.getitem_20[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " tf.repeat_21 (TFOpLambda)   (None, 2, 4, 5, 5, 16)       0         ['tf.__operators__.getitem_21[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " tf.concat_8 (TFOpLambda)    (None, 8, 6, 3, 3, 16)       0         ['tf.repeat_16[0][0]',        \n",
            "                                                                     'compatible_extract_patches_6\n",
            "                                                                    [0][0]',                      \n",
            "                                                                     'tf.repeat_17[0][0]']        \n",
            "                                                                                                  \n",
            " tf.concat_10 (TFOpLambda)   (None, 8, 4, 5, 5, 16)       0         ['tf.repeat_20[0][0]',        \n",
            "                                                                     'compatible_extract_patches_7\n",
            "                                                                    [0][0]',                      \n",
            "                                                                     'tf.repeat_21[0][0]']        \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1  (None, 8, 1, 3, 3, 16)       0         ['tf.concat_8[0][0]']         \n",
            " 8 (SlicingOpLambda)                                                                              \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1  (None, 8, 1, 3, 3, 16)       0         ['tf.concat_8[0][0]']         \n",
            " 9 (SlicingOpLambda)                                                                              \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_2  (None, 8, 1, 5, 5, 16)       0         ['tf.concat_10[0][0]']        \n",
            " 2 (SlicingOpLambda)                                                                              \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_2  (None, 8, 1, 5, 5, 16)       0         ['tf.concat_10[0][0]']        \n",
            " 3 (SlicingOpLambda)                                                                              \n",
            "                                                                                                  \n",
            " tf.repeat_18 (TFOpLambda)   (None, 8, 1, 3, 3, 16)       0         ['tf.__operators__.getitem_18[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " tf.repeat_19 (TFOpLambda)   (None, 8, 1, 3, 3, 16)       0         ['tf.__operators__.getitem_19[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " tf.repeat_22 (TFOpLambda)   (None, 8, 2, 5, 5, 16)       0         ['tf.__operators__.getitem_22[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " tf.repeat_23 (TFOpLambda)   (None, 8, 2, 5, 5, 16)       0         ['tf.__operators__.getitem_23[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " tf.concat_9 (TFOpLambda)    (None, 8, 8, 3, 3, 16)       0         ['tf.repeat_18[0][0]',        \n",
            "                                                                     'tf.concat_8[0][0]',         \n",
            "                                                                     'tf.repeat_19[0][0]']        \n",
            "                                                                                                  \n",
            " tf.concat_11 (TFOpLambda)   (None, 8, 8, 5, 5, 16)       0         ['tf.repeat_22[0][0]',        \n",
            "                                                                     'tf.concat_10[0][0]',        \n",
            "                                                                     'tf.repeat_23[0][0]']        \n",
            "                                                                                                  \n",
            " tf.reshape_37 (TFOpLambda)  (None, 9, 16)                0         ['tf.concat_9[0][0]']         \n",
            "                                                                                                  \n",
            " tf.reshape_44 (TFOpLambda)  (None, 25, 16)               0         ['tf.concat_11[0][0]']        \n",
            "                                                                                                  \n",
            " tf.split_9 (TFOpLambda)     [(None, 9, 8),               0         ['tf.reshape_37[0][0]']       \n",
            "                              (None, 9, 8)]                                                       \n",
            "                                                                                                  \n",
            " tf.split_11 (TFOpLambda)    [(None, 25, 8),              0         ['tf.reshape_44[0][0]']       \n",
            "                              (None, 25, 8)]                                                      \n",
            "                                                                                                  \n",
            " tf.reshape_38 (TFOpLambda)  (None, 9, 4, 2)              0         ['tf.split_9[0][0]']          \n",
            "                                                                                                  \n",
            " tf.reshape_45 (TFOpLambda)  (None, 25, 4, 2)             0         ['tf.split_11[0][0]']         \n",
            "                                                                                                  \n",
            " tf.reshape_36 (TFOpLambda)  (None, 64, 4, 2)             0         ['tf.split_8[0][0]']          \n",
            "                                                                                                  \n",
            " tf.compat.v1.transpose_10   (None, 4, 2, 9)              0         ['tf.reshape_38[0][0]']       \n",
            " (TFOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " tf.reshape_43 (TFOpLambda)  (None, 64, 4, 2)             0         ['tf.split_10[0][0]']         \n",
            "                                                                                                  \n",
            " tf.compat.v1.transpose_12   (None, 4, 2, 25)             0         ['tf.reshape_45[0][0]']       \n",
            " (TFOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " tf.expand_dims_10 (TFOpLam  (None, 64, 4, 1, 2)          0         ['tf.reshape_36[0][0]']       \n",
            " bda)                                                                                             \n",
            "                                                                                                  \n",
            " tf.reshape_39 (TFOpLambda)  (None, 64, 4, 2, 9)          0         ['tf.compat.v1.transpose_10[0]\n",
            "                                                                    [0]']                         \n",
            "                                                                                                  \n",
            " tf.expand_dims_11 (TFOpLam  (None, 64, 4, 1, 2)          0         ['tf.reshape_43[0][0]']       \n",
            " bda)                                                                                             \n",
            "                                                                                                  \n",
            " tf.reshape_46 (TFOpLambda)  (None, 64, 4, 2, 25)         0         ['tf.compat.v1.transpose_12[0]\n",
            "                                                                    [0]']                         \n",
            "                                                                                                  \n",
            " tf.linalg.matmul_8 (TFOpLa  (None, 64, 4, 1, 9)          0         ['tf.expand_dims_10[0][0]',   \n",
            " mbda)                                                               'tf.reshape_39[0][0]']       \n",
            "                                                                                                  \n",
            " tf.linalg.matmul_10 (TFOpL  (None, 64, 4, 1, 25)         0         ['tf.expand_dims_11[0][0]',   \n",
            " ambda)                                                              'tf.reshape_46[0][0]']       \n",
            "                                                                                                  \n",
            " tf.math.multiply_4 (TFOpLa  (None, 64, 4, 1, 9)          0         ['tf.linalg.matmul_8[0][0]']  \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " tf.reshape_40 (TFOpLambda)  (None, 9, 4, 2)              0         ['tf.split_9[0][1]']          \n",
            "                                                                                                  \n",
            " tf.math.multiply_5 (TFOpLa  (None, 64, 4, 1, 25)         0         ['tf.linalg.matmul_10[0][0]'] \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " tf.reshape_47 (TFOpLambda)  (None, 25, 4, 2)             0         ['tf.split_11[0][1]']         \n",
            "                                                                                                  \n",
            " multi_head_relative_positi  (None, 64, 4, 1, 9)          100       ['tf.math.multiply_4[0][0]']  \n",
            " onal_kernel_bias_4 (MultiH                                                                       \n",
            " eadRelativePositionalKerne                                                                       \n",
            " lBias)                                                                                           \n",
            "                                                                                                  \n",
            " tf.compat.v1.transpose_11   (None, 4, 9, 2)              0         ['tf.reshape_40[0][0]']       \n",
            " (TFOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_relative_positi  (None, 64, 4, 1, 25)         324       ['tf.math.multiply_5[0][0]']  \n",
            " onal_kernel_bias_5 (MultiH                                                                       \n",
            " eadRelativePositionalKerne                                                                       \n",
            " lBias)                                                                                           \n",
            "                                                                                                  \n",
            " tf.compat.v1.transpose_13   (None, 4, 25, 2)             0         ['tf.reshape_47[0][0]']       \n",
            " (TFOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " softmax_6 (Softmax)         (None, 64, 4, 1, 9)          0         ['multi_head_relative_position\n",
            "                                                                    al_kernel_bias_4[0][0]']      \n",
            "                                                                                                  \n",
            " tf.reshape_41 (TFOpLambda)  (None, 64, 4, 9, 2)          0         ['tf.compat.v1.transpose_11[0]\n",
            "                                                                    [0]']                         \n",
            "                                                                                                  \n",
            " softmax_7 (Softmax)         (None, 64, 4, 1, 25)         0         ['multi_head_relative_position\n",
            "                                                                    al_kernel_bias_5[0][0]']      \n",
            "                                                                                                  \n",
            " tf.reshape_48 (TFOpLambda)  (None, 64, 4, 25, 2)         0         ['tf.compat.v1.transpose_13[0]\n",
            "                                                                    [0]']                         \n",
            "                                                                                                  \n",
            " tf.linalg.matmul_9 (TFOpLa  (None, 64, 4, 1, 2)          0         ['softmax_6[0][0]',           \n",
            " mbda)                                                               'tf.reshape_41[0][0]']       \n",
            "                                                                                                  \n",
            " tf.linalg.matmul_11 (TFOpL  (None, 64, 4, 1, 2)          0         ['softmax_7[0][0]',           \n",
            " ambda)                                                              'tf.reshape_48[0][0]']       \n",
            "                                                                                                  \n",
            " tf.reshape_42 (TFOpLambda)  (None, 8, 8, 8)              0         ['tf.linalg.matmul_9[0][0]']  \n",
            "                                                                                                  \n",
            " tf.reshape_49 (TFOpLambda)  (None, 8, 8, 8)              0         ['tf.linalg.matmul_11[0][0]'] \n",
            "                                                                                                  \n",
            " dense_9 (Dense)             (None, 8, 8, 8)              72        ['tf.reshape_42[0][0]']       \n",
            "                                                                                                  \n",
            " dense_11 (Dense)            (None, 8, 8, 8)              72        ['tf.reshape_49[0][0]']       \n",
            "                                                                                                  \n",
            " concatenate_10 (Concatenat  (None, 8, 8, 16)             0         ['dense_9[0][0]',             \n",
            " e)                                                                  'dense_11[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)         (None, 8, 8, 16)             0         ['concatenate_10[0][0]']      \n",
            "                                                                                                  \n",
            " multi_Attent1 (Conv2D)      (None, 8, 8, 8)              136       ['dropout_2[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_11 (Concatenat  (None, 8, 8, 16)             0         ['conv1[0][0]',               \n",
            " e)                                                                  'multi_Attent1[0][0]']       \n",
            "                                                                                                  \n",
            " concatenate_12 (Concatenat  (None, 8, 8, 32)             0         ['conv1[0][0]',               \n",
            " e)                                                                  'concatenate_11[0][0]',      \n",
            "                                                                     'tf.math.reduce_sum_3[0][0]']\n",
            "                                                                                                  \n",
            " conv2 (Conv2D)              (None, 8, 8, 32)             1056      ['concatenate_12[0][0]']      \n",
            "                                                                                                  \n",
            " reshape_15 (Reshape)        (None, 32, 64)               0         ['conv2[0][0]']               \n",
            "                                                                                                  \n",
            " squeezer2 (Conv1D)          (None, 32, 64)               4160      ['reshape_15[0][0]']          \n",
            "                                                                                                  \n",
            " reshape_16 (Reshape)        (None, 64, 32)               0         ['squeezer2[0][0]']           \n",
            "                                                                                                  \n",
            " gconv2 (Conv1D)             (None, 64, 32)               1056      ['reshape_16[0][0]']          \n",
            "                                                                                                  \n",
            " reshape_17 (Reshape)        (None, 64, 32)               0         ['gconv2[0][0]']              \n",
            "                                                                                                  \n",
            " unsqueezer2 (Conv1D)        (None, 64, 64)               2112      ['reshape_17[0][0]']          \n",
            "                                                                                                  \n",
            " reshape_18 (Reshape)        (None, 64, 64)               0         ['unsqueezer2[0][0]']         \n",
            "                                                                                                  \n",
            " reshape_19 (Reshape)        (None, 8, 8, 64)             0         ['reshape_18[0][0]']          \n",
            "                                                                                                  \n",
            " dense_12 (Dense)            (None, 8, 8, 192)            12480     ['reshape_19[0][0]']          \n",
            "                                                                                                  \n",
            " dense_14 (Dense)            (None, 8, 8, 192)            12480     ['reshape_19[0][0]']          \n",
            "                                                                                                  \n",
            " tf.split_12 (TFOpLambda)    [(None, 8, 8, 64),           0         ['dense_12[0][0]']            \n",
            "                              (None, 8, 8, 128)]                                                  \n",
            "                                                                                                  \n",
            " tf.split_14 (TFOpLambda)    [(None, 8, 8, 64),           0         ['dense_14[0][0]']            \n",
            "                              (None, 8, 8, 128)]                                                  \n",
            "                                                                                                  \n",
            " compatible_extract_patches  (None, 6, 6, 3, 3, 128)      0         ['tf.split_12[0][1]']         \n",
            " _8 (CompatibleExtractPatch                                                                       \n",
            " es)                                                                                              \n",
            "                                                                                                  \n",
            " compatible_extract_patches  (None, 4, 4, 5, 5, 128)      0         ['tf.split_14[0][1]']         \n",
            " _9 (CompatibleExtractPatch                                                                       \n",
            " es)                                                                                              \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_2  (None, 1, 6, 3, 3, 128)      0         ['compatible_extract_patches_8\n",
            " 4 (SlicingOpLambda)                                                [0][0]']                      \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_2  (None, 1, 6, 3, 3, 128)      0         ['compatible_extract_patches_8\n",
            " 5 (SlicingOpLambda)                                                [0][0]']                      \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_2  (None, 1, 4, 5, 5, 128)      0         ['compatible_extract_patches_9\n",
            " 8 (SlicingOpLambda)                                                [0][0]']                      \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_2  (None, 1, 4, 5, 5, 128)      0         ['compatible_extract_patches_9\n",
            " 9 (SlicingOpLambda)                                                [0][0]']                      \n",
            "                                                                                                  \n",
            " tf.repeat_24 (TFOpLambda)   (None, 1, 6, 3, 3, 128)      0         ['tf.__operators__.getitem_24[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " tf.repeat_25 (TFOpLambda)   (None, 1, 6, 3, 3, 128)      0         ['tf.__operators__.getitem_25[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " tf.repeat_28 (TFOpLambda)   (None, 2, 4, 5, 5, 128)      0         ['tf.__operators__.getitem_28[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " tf.repeat_29 (TFOpLambda)   (None, 2, 4, 5, 5, 128)      0         ['tf.__operators__.getitem_29[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " tf.concat_12 (TFOpLambda)   (None, 8, 6, 3, 3, 128)      0         ['tf.repeat_24[0][0]',        \n",
            "                                                                     'compatible_extract_patches_8\n",
            "                                                                    [0][0]',                      \n",
            "                                                                     'tf.repeat_25[0][0]']        \n",
            "                                                                                                  \n",
            " tf.concat_14 (TFOpLambda)   (None, 8, 4, 5, 5, 128)      0         ['tf.repeat_28[0][0]',        \n",
            "                                                                     'compatible_extract_patches_9\n",
            "                                                                    [0][0]',                      \n",
            "                                                                     'tf.repeat_29[0][0]']        \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_2  (None, 8, 1, 3, 3, 128)      0         ['tf.concat_12[0][0]']        \n",
            " 6 (SlicingOpLambda)                                                                              \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_2  (None, 8, 1, 3, 3, 128)      0         ['tf.concat_12[0][0]']        \n",
            " 7 (SlicingOpLambda)                                                                              \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_3  (None, 8, 1, 5, 5, 128)      0         ['tf.concat_14[0][0]']        \n",
            " 0 (SlicingOpLambda)                                                                              \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_3  (None, 8, 1, 5, 5, 128)      0         ['tf.concat_14[0][0]']        \n",
            " 1 (SlicingOpLambda)                                                                              \n",
            "                                                                                                  \n",
            " tf.repeat_26 (TFOpLambda)   (None, 8, 1, 3, 3, 128)      0         ['tf.__operators__.getitem_26[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " tf.repeat_27 (TFOpLambda)   (None, 8, 1, 3, 3, 128)      0         ['tf.__operators__.getitem_27[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " tf.repeat_30 (TFOpLambda)   (None, 8, 2, 5, 5, 128)      0         ['tf.__operators__.getitem_30[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " tf.repeat_31 (TFOpLambda)   (None, 8, 2, 5, 5, 128)      0         ['tf.__operators__.getitem_31[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " tf.concat_13 (TFOpLambda)   (None, 8, 8, 3, 3, 128)      0         ['tf.repeat_26[0][0]',        \n",
            "                                                                     'tf.concat_12[0][0]',        \n",
            "                                                                     'tf.repeat_27[0][0]']        \n",
            "                                                                                                  \n",
            " tf.concat_15 (TFOpLambda)   (None, 8, 8, 5, 5, 128)      0         ['tf.repeat_30[0][0]',        \n",
            "                                                                     'tf.concat_14[0][0]',        \n",
            "                                                                     'tf.repeat_31[0][0]']        \n",
            "                                                                                                  \n",
            " tf.reshape_51 (TFOpLambda)  (None, 9, 128)               0         ['tf.concat_13[0][0]']        \n",
            "                                                                                                  \n",
            " tf.reshape_58 (TFOpLambda)  (None, 25, 128)              0         ['tf.concat_15[0][0]']        \n",
            "                                                                                                  \n",
            " tf.split_13 (TFOpLambda)    [(None, 9, 64),              0         ['tf.reshape_51[0][0]']       \n",
            "                              (None, 9, 64)]                                                      \n",
            "                                                                                                  \n",
            " tf.split_15 (TFOpLambda)    [(None, 25, 64),             0         ['tf.reshape_58[0][0]']       \n",
            "                              (None, 25, 64)]                                                     \n",
            "                                                                                                  \n",
            " tf.reshape_52 (TFOpLambda)  (None, 9, 4, 16)             0         ['tf.split_13[0][0]']         \n",
            "                                                                                                  \n",
            " tf.reshape_59 (TFOpLambda)  (None, 25, 4, 16)            0         ['tf.split_15[0][0]']         \n",
            "                                                                                                  \n",
            " tf.reshape_50 (TFOpLambda)  (None, 64, 4, 16)            0         ['tf.split_12[0][0]']         \n",
            "                                                                                                  \n",
            " tf.compat.v1.transpose_14   (None, 4, 16, 9)             0         ['tf.reshape_52[0][0]']       \n",
            " (TFOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " tf.reshape_57 (TFOpLambda)  (None, 64, 4, 16)            0         ['tf.split_14[0][0]']         \n",
            "                                                                                                  \n",
            " tf.compat.v1.transpose_16   (None, 4, 16, 25)            0         ['tf.reshape_59[0][0]']       \n",
            " (TFOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " tf.expand_dims_12 (TFOpLam  (None, 64, 4, 1, 16)         0         ['tf.reshape_50[0][0]']       \n",
            " bda)                                                                                             \n",
            "                                                                                                  \n",
            " tf.reshape_53 (TFOpLambda)  (None, 64, 4, 16, 9)         0         ['tf.compat.v1.transpose_14[0]\n",
            "                                                                    [0]']                         \n",
            "                                                                                                  \n",
            " tf.expand_dims_13 (TFOpLam  (None, 64, 4, 1, 16)         0         ['tf.reshape_57[0][0]']       \n",
            " bda)                                                                                             \n",
            "                                                                                                  \n",
            " tf.reshape_60 (TFOpLambda)  (None, 64, 4, 16, 25)        0         ['tf.compat.v1.transpose_16[0]\n",
            "                                                                    [0]']                         \n",
            "                                                                                                  \n",
            " tf.linalg.matmul_12 (TFOpL  (None, 64, 4, 1, 9)          0         ['tf.expand_dims_12[0][0]',   \n",
            " ambda)                                                              'tf.reshape_53[0][0]']       \n",
            "                                                                                                  \n",
            " tf.linalg.matmul_14 (TFOpL  (None, 64, 4, 1, 25)         0         ['tf.expand_dims_13[0][0]',   \n",
            " ambda)                                                              'tf.reshape_60[0][0]']       \n",
            "                                                                                                  \n",
            " tf.math.multiply_6 (TFOpLa  (None, 64, 4, 1, 9)          0         ['tf.linalg.matmul_12[0][0]'] \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " tf.reshape_54 (TFOpLambda)  (None, 9, 4, 16)             0         ['tf.split_13[0][1]']         \n",
            "                                                                                                  \n",
            " tf.math.multiply_7 (TFOpLa  (None, 64, 4, 1, 25)         0         ['tf.linalg.matmul_14[0][0]'] \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " tf.reshape_61 (TFOpLambda)  (None, 25, 4, 16)            0         ['tf.split_15[0][1]']         \n",
            "                                                                                                  \n",
            " multi_head_relative_positi  (None, 64, 4, 1, 9)          100       ['tf.math.multiply_6[0][0]']  \n",
            " onal_kernel_bias_6 (MultiH                                                                       \n",
            " eadRelativePositionalKerne                                                                       \n",
            " lBias)                                                                                           \n",
            "                                                                                                  \n",
            " tf.compat.v1.transpose_15   (None, 4, 9, 16)             0         ['tf.reshape_54[0][0]']       \n",
            " (TFOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_relative_positi  (None, 64, 4, 1, 25)         324       ['tf.math.multiply_7[0][0]']  \n",
            " onal_kernel_bias_7 (MultiH                                                                       \n",
            " eadRelativePositionalKerne                                                                       \n",
            " lBias)                                                                                           \n",
            "                                                                                                  \n",
            " tf.compat.v1.transpose_17   (None, 4, 25, 16)            0         ['tf.reshape_61[0][0]']       \n",
            " (TFOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " softmax_8 (Softmax)         (None, 64, 4, 1, 9)          0         ['multi_head_relative_position\n",
            "                                                                    al_kernel_bias_6[0][0]']      \n",
            "                                                                                                  \n",
            " tf.reshape_55 (TFOpLambda)  (None, 64, 4, 9, 16)         0         ['tf.compat.v1.transpose_15[0]\n",
            "                                                                    [0]']                         \n",
            "                                                                                                  \n",
            " softmax_9 (Softmax)         (None, 64, 4, 1, 25)         0         ['multi_head_relative_position\n",
            "                                                                    al_kernel_bias_7[0][0]']      \n",
            "                                                                                                  \n",
            " tf.reshape_62 (TFOpLambda)  (None, 64, 4, 25, 16)        0         ['tf.compat.v1.transpose_17[0]\n",
            "                                                                    [0]']                         \n",
            "                                                                                                  \n",
            " tf.linalg.matmul_13 (TFOpL  (None, 64, 4, 1, 16)         0         ['softmax_8[0][0]',           \n",
            " ambda)                                                              'tf.reshape_55[0][0]']       \n",
            "                                                                                                  \n",
            " tf.linalg.matmul_15 (TFOpL  (None, 64, 4, 1, 16)         0         ['softmax_9[0][0]',           \n",
            " ambda)                                                              'tf.reshape_62[0][0]']       \n",
            "                                                                                                  \n",
            " tf.reshape_56 (TFOpLambda)  (None, 8, 8, 64)             0         ['tf.linalg.matmul_13[0][0]'] \n",
            "                                                                                                  \n",
            " tf.reshape_63 (TFOpLambda)  (None, 8, 8, 64)             0         ['tf.linalg.matmul_15[0][0]'] \n",
            "                                                                                                  \n",
            " dense_13 (Dense)            (None, 8, 8, 64)             4160      ['tf.reshape_56[0][0]']       \n",
            "                                                                                                  \n",
            " dense_15 (Dense)            (None, 8, 8, 64)             4160      ['tf.reshape_63[0][0]']       \n",
            "                                                                                                  \n",
            " concatenate_13 (Concatenat  (None, 8, 8, 128)            0         ['dense_13[0][0]',            \n",
            " e)                                                                  'dense_15[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)         (None, 8, 8, 128)            0         ['concatenate_13[0][0]']      \n",
            "                                                                                                  \n",
            " concatenate_14 (Concatenat  (None, 8, 8, 96)             0         ['conv2[0][0]',               \n",
            " e)                                                                  'reshape_19[0][0]']          \n",
            "                                                                                                  \n",
            " multi_Attent2 (Conv2D)      (None, 8, 8, 8)              1032      ['dropout_3[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_15 (Concatenat  (None, 8, 8, 136)            0         ['conv2[0][0]',               \n",
            " e)                                                                  'concatenate_14[0][0]',      \n",
            "                                                                     'multi_Attent2[0][0]']       \n",
            "                                                                                                  \n",
            " conv3 (Conv2D)              (None, 8, 8, 32)             4384      ['concatenate_15[0][0]']      \n",
            "                                                                                                  \n",
            " conv4 (Conv2D)              (None, 6, 6, 64)             18496     ['conv3[0][0]']               \n",
            "                                                                                                  \n",
            " conv5 (Conv2D)              (None, 6, 6, 128)            8320      ['conv4[0][0]']               \n",
            "                                                                                                  \n",
            " global_average_pooling2d (  (None, 128)                  0         ['conv5[0][0]']               \n",
            " GlobalAveragePooling2D)                                                                          \n",
            "                                                                                                  \n",
            " fc1 (Dense)                 (None, 100)                  12900     ['global_average_pooling2d[0][\n",
            "                                                                    0]']                          \n",
            "                                                                                                  \n",
            " fc2 (Dense)                 (None, 20)                   2020      ['fc1[0][0]']                 \n",
            "                                                                                                  \n",
            " fc3 (Dense)                 (None, 7)                    147       ['fc2[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98346 (384.16 KB)\n",
            "Trainable params: 98226 (383.70 KB)\n",
            "Non-trainable params: 120 (480.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def AttentionGCN():\n",
        "\n",
        "#     input_layer = Input(shape=(8,8,17), name='input')\n",
        "#     #inputs = data_augmentation(input_layer)\n",
        "#     conv1 = Conv2D(8, kernel_size=1, activation='relu', name='conv1', kernel_regularizer=regularizers.l2(0.0005))(input_layer)\n",
        "#     nn = attention_layers.cot_attention(conv1, kernel_size=3)\n",
        "#     #pool1 = MaxPooling2D(pool_size=(2,2), name='pool1')(nn)\n",
        "\n",
        "# ########## Graph-based global reasoning module # input is 8x8x8\n",
        "\n",
        "#     graph_t = Reshape((8, 8*8))(conv1)\n",
        "#     squeezed_graph_t = Conv1D(16, 1, activation='relu', name='squeezer', kernel_regularizer=regularizers.l2(0.0001))(graph_t)\n",
        "#     squeezed_graph = Reshape((8, 16))(squeezed_graph_t)\n",
        "#     gconv = Conv1D(64, 1, activation='relu', name='gconv', kernel_regularizer=regularizers.l2(0.0001))(squeezed_graph)\n",
        "#     gconv_t = Reshape((8, 64))(gconv)\n",
        "#     unsqueezed_graph_transpose = Conv1D(64, 1, activation='relu', name='unsqueezer', kernel_regularizer=regularizers.l2(0.0001))(gconv_t)\n",
        "#     unsqueezed_graph = Reshape((64, 8))(unsqueezed_graph_transpose)\n",
        "#     glore = Reshape((8, 8, 8))(unsqueezed_graph)\n",
        "\n",
        "#     nn1 =  attention_layers.neighborhood_attention(glore, kernel_size=3)\n",
        "\n",
        "#     block1 = layers.concatenate([conv1, nn1], axis=-1) #################  you do not need to add the feature maps of the conv1 layer\n",
        "#     block12 = layers.concatenate([conv1, block1, nn], axis=-1) #################  you do not need to use the feature maps of the attention block\n",
        "#     #pool2 = MaxPooling2D(pool_size=(2,2), name='pool2')(block1)\n",
        "#     conv2 = Conv2D(32, kernel_size=1, activation='relu', name='conv2', kernel_regularizer=regularizers.l2(0.0005))(block12)\n",
        "\n",
        "# # Graph-based global reasoning module 2 # input is 8x8x32\n",
        "\n",
        "#     graph_t2 = Reshape((32,8*8))(conv2)\n",
        "#     squeezed_graph_t2 = Conv1D(64, 1, activation='relu', name='squeezer2', kernel_regularizer=regularizers.l2(0.0001))(graph_t2)\n",
        "#     squeezed_graph2 = Reshape((64, 32))(squeezed_graph_t2)\n",
        "#     gconv2 = Conv1D(32, 1, activation='relu', name='gconv2', kernel_regularizer=regularizers.l2(0.0001))(squeezed_graph2)\n",
        "#     gconv_transpose2 = Reshape((64, 32))(gconv2)\n",
        "#     unsqueezed_graph_t2 = Conv1D(8*8, 1, activation='relu', name='unsqueezer2', kernel_regularizer=regularizers.l2(0.0001))(gconv_transpose2)\n",
        "#     unsqueezed_graph2 = Reshape((8*8, 64))(unsqueezed_graph_t2)\n",
        "#     glore2 = Reshape((8, 8, 64))(unsqueezed_graph2)\n",
        "\n",
        "#     nn2 =  attention_layers.neighborhood_attention(glore2, kernel_size=3)\n",
        "\n",
        "#     block2 = layers.concatenate([conv2, nn2], axis=-1) #################  you do not need to add the feature maps of the conv2 layer\n",
        "#     block21 = layers.concatenate([conv2, block2, nn], axis=-1) #################  you do not need to use the feature maps of the attention block2\n",
        "\n",
        "#     conv3 = Conv2D(32, kernel_size=1, activation='relu', name='conv3', kernel_regularizer=regularizers.l2(0.0005))(block21)\n",
        "#     conv4 = Conv2D(64, kernel_size=3, activation='relu', name='conv4', kernel_regularizer=regularizers.l2(0.0005))(conv3)\n",
        "#     conv5 = Conv2D(128, kernel_size=1, activation='relu', name='conv5', kernel_regularizer=regularizers.l2(0.0005))(conv4)\n",
        "\n",
        "#     # flat = Flatten()(conv5)\n",
        "#     gap = GlobalAveragePooling2D()(conv5)\n",
        "\n",
        "#     fc1 = Dense(100, activation='relu', name='fc1', kernel_regularizer=regularizers.l2(0.01))(gap)\n",
        "#     fc2 = Dense(20, activation='relu', name='fc2', kernel_regularizer=regularizers.l2(0.01))(fc1)\n",
        "#     fc3 = Dense(7, activation='softmax', name='fc3', kernel_regularizer=regularizers.l2(0.0001))(fc2)\n",
        "\n",
        "#     model = Model(inputs=input_layer, outputs=fc3, name='CNN_model_with_global_reasoning_module')\n",
        "\n",
        "#     return model\n",
        "\n",
        "# # define the model with input layer and output layer\n",
        "# model = AttentionGCN()"
      ],
      "metadata": {
        "id": "8YVKFZUKWlSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compiling the model\n",
        "adam = Adam(learning_rate=0.002)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "# checkpoint\n",
        "filepath = \"/content/drive/MyDrive/model/AttGCN_Hsr_Augsburg.h5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "history = model.fit(x=XtrainS, y=YtrainS, batch_size=256, epochs=100, callbacks=callbacks_list)\n",
        "model.save(\"/content/drive/MyDrive/model/AttGCN_Hsr_Augsburg.h5\")"
      ],
      "metadata": {
        "id": "gPOFXowkWn25",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmETgiMPyJ0p",
        "outputId": "bac326b9-c650-4fc6-9c1b-8833370e8dce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "881"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Xtest = XtestS.reshape(-1, windowSize, windowSize, 17)\n",
        "ytest = utils.to_categorical(YtestS)\n",
        "y_pred_test = model.predict(Xtest)\n",
        "y_pred_test = np.argmax(y_pred_test, axis=1)\n",
        "kappa = cohen_kappa_score(YtestS, y_pred_test)\n",
        "print(\"Kappa score : %.5f\" % kappa)\n",
        "ca = np.sum(y_pred_test == np.argmax(ytest, axis=1)) / ytest.shape[0]\n",
        "print(\"Classification accuracy: %.5f\" % ca)\n",
        "classification = classification_report(np.argmax(ytest, axis=1), y_pred_test)\n",
        "print(classification)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Em7gL_wByNl1",
        "outputId": "e51b5372-1157-4377-d72c-1e8915b4d941"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2423/2423 [==============================] - 21s 8ms/step\n",
            "Kappa score : 0.84058\n",
            "Classification accuracy: 0.88889\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.95      0.95     13361\n",
            "           1       0.90      0.96      0.93     30065\n",
            "           2       0.59      0.59      0.59      3830\n",
            "           3       0.97      0.90      0.94     26609\n",
            "           4       0.14      0.56      0.22       523\n",
            "           5       0.26      0.03      0.05      1638\n",
            "           6       0.56      0.43      0.49      1507\n",
            "\n",
            "    accuracy                           0.89     77533\n",
            "   macro avg       0.63      0.63      0.60     77533\n",
            "weighted avg       0.89      0.89      0.89     77533\n",
            "\n"
          ]
        }
      ]
    }
  ]
}